# 云原生

mountpath hostpath



## Kubernetes核心实战

### 常用命令

```sh
kubectl get nodes -o wide

kubectl apply -f xx.yml
kubectl create -f xx.yml
kubectl delete -f xx.yml
```



```sh
# Namespace
kubectl create ns hello
```

```sh
# pod
kubectl run mynginx --image=nginx
kubectl exec -it redis -- redis-cli

# 查看default名称空间的Pod
kubectl get pod 
# 描述
kubectl describe pod <-n 命名空间名称> Pod名字
# 删除
kubectl delete pod Pod名字
# 查看Pod的运行日志
kubectl logs Pod名字

# 每个Pod - k8s都会分配一个ip
kubectl get pod -owide
```

```sh
# Deployment
kubectl create deployment my-dep --image=nginx --replicas=3

# 扩缩容
kubectl scale --replicas=5 deployment/my-dep
```

```sh
# Service
kubectl expose deployment my-dep --port=8000 --target-port=80

# ClusterIP模式
kubectl expose deployment my-dep --port=8000 --target-port=80 --type=ClusterIP
通过 服务名.所在名称空间.svc:port 在集群内部访问，如 my-dep.default.svc:8000

# NodePort模式
kubectl expose deployment my-dep --port=8000 --target-port=80 --type=NodePort
通过 公网IP:节点port 在集群外也可以访问，如 公网IP:30948

kubectl get svc
```

```sh
# ingress
kubectl get ing
```

```sh
# PV&PVC
kubectl get pv,pvc
```

```sh
# ConfigMap
kubectl create cm redis-conf --from-file=redis.conf
kubectl get cm
kubectl get cm redis-conf --oyaml
kubectl edit cm redis-conf
```

```sh
# secret
kubectl get secret
kubectl get secret xxx -oyaml

kubectl create secret docker-registry <自定义名称> \
  --docker-server=<你的镜像仓库服务器> \
  --docker-username=<你的用户名> \
  --docker-password=<你的密码> \
  --docker-email=<你的邮箱地址>
```



```sh
kubectl top nodes
kubectl top pods

kubectl cp started.html nginx-po:/usr/share/nginx/html/
```

### 架构

Kubernetes **Cluster** **=** N **Master** Node **+** N **Worker** Node：N主节点+N工作节点； N>=1

- **控制平面组件（Control Plane Components）** 
  - **kube-apiserver** 该组件公开了 Kubernetes API。 API 服务器是 Kubernetes 控制面的前端。
  - **etcd** 键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。
  - **kube-scheduler** 负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。
  - **kube-controller-manager** 
  - **cloud-controller-manager** 允许您链接集群到云提供商的应用编程接口中， 并把和该云平台交互的组件与只和您的集群交互的组件分离开。

- **Node 组件** 节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。
  - **kubelet** 保证容器（containers）都 运行在 Pod中。
  - **kube-proxy** 集群中每个节点上运行的网络代理。

![image.png](img_%E4%BA%91%E5%8E%9F%E7%94%9F/1626605698082-bf4351dd-6751-44b7-aaf7-7608c847ea42.png)









Kubernetes 自定义资源定义（Custom Resource Definition，简称 CRD）是一种强大的 Kubernetes API 扩展机制。它允许用户创建和管理自定义资源，这些资源不是 Kubernetes 标准 API 的一部分。CRD 使得 Kubernetes 不仅限于内建资源（如 Pod、Service 等），还可以支持用户定义的资源类型。

### 安装

#### Kubernetes安装流程

1. 提前为所有机器安装Docker

2. 安装kubeadm

   1. 基础环境
   2. 每个节点安装安装 kubelet、kubeadm，主节点安装 kubectl

3. 使用kubeadm引导集群

   1.  下载各个机器需要的镜像 

   2. 主节点：初始化主节点 kubeadm init

   3. 每个工作节点：加入工作节点 kubeadm join

4. 部署dashboard

   1. 部署

   2. 设置访问端口，创建访问账号 ServiceAccount ClusterRole ClusterRoleBinding `kubectl apply -f dash.yaml`

   3. 令牌访问 #获取访问令牌
      `kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"`

   4. 界面

      

#### kubectl

kubectl 是一个命令行客户端工具。它的作用是与 Kubernetes **API Server**（运行在主节点上的一个核心组件）进行通信，发送指令（如创建 Pod、查看服务状态等）。

**主节点 (Master Node)**：主节点拥有访问 API Server 所必需的配置文件 (`$HOME/.kube/config`) 和证书。这个配置文件包含了 API Server 的地址、端口、以及认证信息（如证书、令牌等），所以可以在主节点上安装了 kubectl 直接运行 `kubectl get pods`



**最佳实践是：不要直接在主节点或工作节点上使用 `kubectl`来管理集群。**

通常的建议是：

1. **在你的本地笔记本电脑或一台专门的跳板机/管理机上安装 `kubectl`**。
2. 将主节点上的 `$HOME/.kube/config`文件**安全地复制**到你的本地机器的 `$HOME/.kube/`目录下。
3. 从此，你可以在本地直接使用 `kubectl`命令来管理远端的 Kubernetes 集群。





架构安装参考：

[Kubernetes基础概念](https://www.yuque.com/atguigu-team/frzi7z/ghnb83#LmE9h)



### 资源和对象

配置文件是描述资源属性的文件，对象是资源的实例

**资源**

- 元数据

- 集群

  - Namespace
  - Node
  - ClusterRole
  - ClusterRoleBinding

- 命名空间

  - POD：容器组，包含一个或多个共享资源的容器，每个pod存在底层容器pause，其他容器依赖pause，实现网络、存储资源等共享

    - 副本 replicas
    - 控制器
      - 无状态服务 Deployment
        
        - 一个 Deployment 不包含多个不同的 Pod，它只包含多个相同的 Pod 副本
        
        - 创建 Replica Set / Pod；滚动升级/回滚；平滑扩容和缩容；暂停与恢复 Deployment
        
      - 有状态服务 StatefulSet

        - 稳定的持久化存储；稳定的网络标志；有序部署，有序扩展，有序收缩，有序删除
        - 组成：Headless Service；volumeClaimTemplate

      - 守护进程

      - 任务/定时任务

  - 存储和配置

  - 其他

    - Role 权限组，命名空间级权限

    - RoleBinding



**对象**

- Spec 规格，描述对象的期望状态
- Status 状态，对象实际状态



### POD

```yml
apiVersion: v1 # api 文档版本
kind: Pod  # 资源对象类型，也可以配置为像Deployment、StatefulSet这一类的对象
metadata: # Pod 相关的元数据，用于描述 Pod 的数据
  name: nginx-demo # Pod 的名称
  labels: # 定义 Pod 的标签
    type: app # 自定义 label 标签，名字为 type，值为 app
    test: 1.0.0 # 自定义 label 标签，描述 Pod 版本号
  namespace: 'default' # 命名空间的配置
spec: # 期望 Pod 按照这里面的描述进行创建
  containers: # 对于 Pod 中的容器描述
  - name: nginx # 容器的名称
    image: nginx:1.7.9 # 指定容器的镜像
    imagePullPolicy: IfNotPresent # 镜像拉取策略，指定如果本地有就用本地的，如果没有就拉取远程的
    command: # 指定容器启动时执行的命令
    - nginx
    - -g
    - 'daemon off;' # nginx -g 'daemon off;'
    workingDir: /usr/share/nginx/html # 定义容器启动后的工作目录
    ports:
    - name: http # 端口名称
      containerPort: 80 # 描述容器内要暴露什么端口
      protocol: TCP # 描述该端口是基于哪种协议通信的
    - env: # 环境变量
      name: JVM_OPTS # 环境变量名称
      value: '-Xms128m -Xmx128m' # 环境变量的值
    reousrces:
      requests: # 最少需要多少资源
        cpu: 100m # 限制 cpu 最少使用 0.1 个核心
        memory: 128Mi # 限制内存最少使用 128兆
      limits: # 最多可以用多少资源
        cpu: 200m # 限制 cpu 最多使用 0.2 个核心
        memory: 256Mi # 限制 最多使用 256兆
  restartPolicy: OnFailure # 重启策略，只有失败的情况才会重启
```

#### 探针

**类型**

- StartupProbe 判断应用程序是否已经启动了。当配置了 startupProbe 后，会先禁用其他探针，直到 startupProbe 成功后，其他探针才会继续。

  作用：由于有时候不能准确预估应用一定是多长时间启动成功，因此配置另外两种方式不方便配置初始化时长来检测，而配置了 statupProbe 后，只有在应用启动成功了，才会执行另外两种探针，可以更加方便的结合使用另外两种探针使用。

  ```yml
  startupProbe:
    httpGet:
      path: /api/startup
      port: 80
  ```

- LivenessProbe

  用于探测容器中的应用是否运行，如果探测失败，kubelet 会根据配置的重启策略进行重启，若没有配置，默认就认为容器启动成功，不会执行重启策略。

  ```yml
  livenessProbe:
    failureThreshold: 5
    httpGet:
      path: /health
      port: 8080
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 5
  ```

- ReadinessProbe

  用于探测容器内的程序是否健康，它的返回值如果返回 success，那么就认为该容器已经完全启动，并且该容器是可以接收外部流量的。

  ```yml
  readinessProbe:
    failureThreshold: 3 # 错误次数
    httpGet:
      path: /ready
      port: 8181
      scheme: HTTP
    periodSeconds: 10 # 间隔时间
    successThreshold: 1
    timeoutSeconds: 1
  ```

  

**探测方式**

- ExecAction 在容器内部执行一个命令，如果返回值为 0，则任务容器时健康的。

  ```yml
  实际执行命令为：sh -c sleep 5;echo success > /inited
  livenessProbe:
    exec:
      command:
        - sh
        - -c
        - sleep 5;echo success > /inited
  ```

- TCPSocketAction 通过 tcp 连接监测容器内端口是否开放，如果开放则证明该容器健康

  ```yml
  livenessProbe:
    tcpSocket:
      port: 80
  ```

- HTTPGetAction 生产环境用的较多的方式，发送 HTTP 请求到容器内的应用程序，如果接口返回的状态码在 200~400 之间，则认为容器健康。

  ```yml
  livenessProbe:
    failureThreshold: 5
    httpGet:
      path: /health
      port: 8080
      scheme: HTTP
      httpHeaders:
        - name: xxx
          value: xxx
  ```

**参数配置**

```
initialDelaySeconds: 60 # 初始化时间
timeoutSeconds: 2 # 超时时间
periodSeconds: 5 # 监测间隔时间
successThreshold: 1 # 检查 1 次成功就表示成功
failureThreshold: 2 # 监测失败 2 次就表示失败
```

#### 生命周期

```yml
lifecycle:
  postStart: # 容创建完成后执行的动作，不能保证该操作一定在容器的 command 之前执行，一般不使用
    exec: # 可以是 exec / httpGet / tcpSocket
      command:
        - sh
        - -c
        - 'mkdir /data'
  preStop: # 在容器停止前执行的动作
    httpGet: # 发送一个 http 请求
      path: /
      port: 80
    exec: # 执行一个命令
      command:
        - sh
        - -c
        - sleep 9
```

#### Label 和 Selector

- 标签（Label）

  - 配置文件：在各类资源的 metadata.labels 中进行配置

  - kubectl

    ```sh
    # 临时创建 label
    kubectl label po  app=hello
    
    # 修改已经存在的标签
    kubectl label po  app=hello2 --overwrite
    
    # 查看 label
    # selector 按照 label 单值查找节点
    kubectl get po -A -l app=hello
    # 查看所有节点的 labels
    kubectl get po --show-labels
    ```

- 选择器（Selector）

  - 配置文件：在各对象的配置 spec.selector 或其他可以写 selector 的属性中编写

  - kubectl

    ```sh
    # 匹配单个值，查找 app=hello 的 pod
    kubectl get po -A -l app=hello
    # 匹配多个值
    kubectl get po -A -l 'k8s-app in (metrics-server, kubernetes-dashboard)'
    或 
    # 查找 version!=1 and app=nginx 的 pod 信息
    kubectl get po -l version!=1,app=nginx
    # 不等值 + 语句
    kubectl get po -A -l version!=1,'app in (busybox, nginx)'
    ```

    





### 工作负载

![image-20251014084820970](img_%E4%BA%91%E5%8E%9F%E7%94%9F/image-20251014084820970.png)

<img src="img_%E4%BA%91%E5%8E%9F%E7%94%9F/image-20251014085045170.png" alt="image-20251014085045170"  />



### 存储和配置

存储

- volumn 数据卷，容器数据持久化，数据共享

- CSI 接口规范，暴露存储系统给容器化应用程序



配置

- ConfigMap
- Secret
- downwardAPI 将pod信息注入容器内部



```yml
apiVersion: v1
data:    #data是所有真正的数据，key：默认是文件名，对应下面volumes中key   value：配置文件的内容
  redis.conf: |
    appendonly yes
kind: ConfigMap
metadata:
  name: redis-conf-configmap-name
  namespace: default
  
  
  
apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis
    command:
      - redis-server
      - "/redis-master/redis.conf"  #指的是redis容器内部的位置
    ports:
    - containerPort: 6379
    volumeMounts:
    - mountPath: /data
      name: data
    - mountPath: /redis-master
      name: config
  volumes:
    - name: data
      emptyDir: {}
    - name: config
      configMap:
        name: redis-conf-configmap-name
        items:
        - key: redis.conf
          path: redis.conf  # 路径为/redis-master/redis.conf
```



### Prometheus

#### Prometheus 安装使用

```yml
-- yml配置

# 全局配置
global:
  scrape_interval: 15s
  evaluation_interval: 15s

# 规则
rule_files:
  # 规则文件路径
  - "/etc/prometheus/rules/*_rules.yml"
  - "/etc/prometheus/rules/*_alerts.yml"

# 监控目标
scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]  # 监控Prometheus自身

  # 采集node exporter监控数据
  - job_name: "node"
    static_configs:
      - targets: ["localhost:9100"]  # Node Exporter默认端口

alerting:
  alertmanagers:
    - static_configs:
        - targets: ["localhost:9093"]  # Alertmanager服务地址
```

#### Exporter

- 从其他软件或系统中获取状态信息，并转换为 Prometheus metrics
- 软件本身通过 url 暴露 Prometheus 格式的 metrics

#### Node Exporter

```sh
# 下载Node Exporter（ARM64架构版本）
$ wget https://github.com/prometheus/node_exporter/releases/download/v1.3.0/node_exporter-1.3.0.linux-arm64.tar.gz

# 解压安装包
$ tar zxf node_exporter-1.3.0.linux-arm64.tar.gz

# 进入解压后的目录
$ cd node_exporter-1.3.0.linux-arm64

# 启动Node Exporter服务（默认监听9100端口）
$ ./node_exporter
```



#### PromQL 

- 瞬时向量 (Instant vector)
  一组时间序列，每个时间序列包含单个样本，它们共享相同的时间戳。也就是说，表达式的返回值中只会包含该时间序列中的最新的一个样本值。

  ```
  http_requests_total
  {__name__="http_requests_total"}
  ```

  

- 区间向量 (Range vector)
  一组时间序列，每个时间序列包含一段时间范围内的样本数据。

  ```
  http_requests_total{job="prometheus", code="200"}
  http_requests_total{status_code=~"2.*"}
  ```

  

- 标量 (Scalar)
  浮点型的数据值。

- 字符串 (String)

##### 指标类型

- Counter（计数器）：只增不减
  服务的请求数、已完成的任务数、错误发生的次数
- Guage（仪表盘） ：可随意变化
  进程数量、 内存使用率、温度、 并发请求的数量

- Histogram
  样本的值分布在 bucket 中的数量，命名为<basename>_bucket{le="<上边界>"}

- Summary
  server 端直接算好分位数，不能聚合

#### Alertmanger

```yml
# alertmanager.yml
global:
  smtp_smarthost: 'localhost:25'
  smtp_from: 'alertmanager@example.com'
  smtp_require_tls: false

templates:
- '/etc/alertmanager/template/*.tmpl'

route:
  receiver: 'email'
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  routes:
    - receiver: 'slack'
      group_wait: 10s
    - service=="mysql|cassandra"
      - receiver: 'webhook'
      group_by: [product, environment]
      - team="frontend"

receivers:
  - name: 'email'
    email_configs:
      - to: 'alerts@example.com'
  - name: 'slack'
    slack_configs:
      - channel: 'xxxx'
  - name: 'webhook'
    webhook_configs:
      - url: 'xxxx'
```

##### 配置告警规则

```yml
# /etc/prometheus/rules/node_alerts.yml
groups:
- name: node_alerts
  rules:
  - alert: HostOutOfMemory
    expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 > 10
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: Host out of memory (instance {{ $labels.instance }})
      description: "Node memory is filling up (> 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
```

- 节点的内存可用率不断变化，每隔一段时间由scrape_interval 定义的时间被 Prometheus抓取一次，默认是 15 秒。
- 根据每个 evaluation_interval 的指标来评估告警规则，默认是 15 秒。
- 当告警表达式为 true 时，会创建一个告警并转换到 Pending 状态，执行 for 语句。
- 在下一个评估周期中，如果告警表达式仍然为true，则检查 for 的持续时间。如果超过了持续时间，则告警将转换为 Firing，生成通知并将其推送到 Alertmanager。
- 如果告警表达式不再为 true，则 Prometheus会将告警规则的状态从 Pending 更改为Inactive。

#### Prometheus Operator

Kubernetes 原生的 **控制器（Operator）**，专注于通过 CRD（自定义资源）自动化管理 Prometheus 及其相关组件（如 Alertmanager、ServiceMonitor）的生命周期。

- Prometheus CRD： 声明 Prometheus deployment 期望的状态。
- Prometheus Server ：Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的 PrometheusServer 集群。
- Alertmanager CRD： 声明了 Alertmanager Deployment。
- ServiceMonitor CRD： 声明式指定应如何监控 Kubernetes Service，自动生成相关 Prometheus 抓取配置。
- Operator： 根据自定义资源（CRD）来部署和管理 Prometheus Server，同时监控这些自定义资源事件的变化来做相应的处理，是整个系统的控制中心。

#### Kube - prometheus

Kube - prometheus 组件安装

基于 Prometheus Operator 的 **一站式监控套件**，提供开箱即用的完整监控解决方案。

- Prometheus Operator：创建 CRD 自定义的资源对象
- Highly available Prometheus：创建高可用的 Prometheus
- Highly available Alertmanager：创建高可用的告警组件
- Prometheus node-exporter：创建主机的监控组件
- Prometheus Adapter for Kubernetes Metrics APIs：创建自定义监控的指标工具（例如可以通过 Nginx 的 request 来进行应用的自动伸缩）
- kube-state-metrics：监控 K8s 相关资源对象的状态指标
- Grafana：监控面板



**Exporter**

Exporter是Prometheus Operator的组件之一，用于采集非云原生应用的监控数据。它通过DaemonSet部署在集群中，监听目标主机的9100端口（默认Node Exporter端口），定期向Prometheus Server推送数据。 ‌

**ServiceMonitor**

ServiceMonitor是Prometheus Operator的核心组件，用于自动发现并监控Kubernetes中的服务。它通过分析Service对象和Endpoints标签，动态生成监控配置，无需手动配置Prometheus的静态目标。 ‌

**区别**

- ‌**Exporter**‌：针对非云原生应用（如传统应用），**通过HTTP接口推送数据**。 ‌
- ‌**ServiceMonitor**‌：仅用于云原生应用（如Kubernetes服务），基于服务发现机制**自动配置监控目标**。





```yml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: k8s
  namespace: monitoring
spec:
  alerting:
    alertmanagers:
    - apiVersion: v2
      name: alertmanager-main						///
      namespace: monitoring
      port: web
  enableFeatures: []
  externalLabels: {}
  image: quay.io/prometheus/prometheus:v2.31.1
  nodeSelector:
    kubernetes.io/os: linux
  podMonitorNamespaceSelector: {}
  podMonitorSelector: {}
  probeNamespaceSelector: {}
  probeSelector: {}
  replicas: 2
  resources:
    requests:
      memory: 400Mi
  ruleNamespaceSelector: {}
  ruleSelector: {}					///
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: prometheus-k8s
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}				///
  version: 2.31.1
```

```yml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/name: kube-controller-manager
    app.kubernetes.io/part-of: kube-prometheus
  name: kube-controller-manager
  namespace: monitoring
spec:
  endpoints:
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    interval: 30s
    metricRelabelings: []  # 原图此处可能有未展开的配置
    port: https-metrics
    scheme: https
    tlsConfig:
      insecureSkipVerify: true
  jobLabel: app.kubernetes.io/name
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      app.kubernetes.io/name: kube-controller-manager
```

```yml
# Service 定义（ClusterIP 类型，无头服务）
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-controller-manager
  labels:
    app.kubernetes.io/name: kube-controller-manager
spec:
  type: ClusterIP
  clusterIP: None  # 无头服务（Headless Service）
  ports:
    - name: https-metrics
      port: 10257
      targetPort: 10257
      protocol: TCP

---
# Endpoints 定义（显式指定后端实例）
apiVersion: v1
kind: Endpoints
metadata:
  labels:
    app.kubernetes.io/name: kube-controller-manager
  name: kube-controller-manager
  namespace: kube-system
subsets:
  - addresses:
      - ip: 192.168.0.5  # 后端 Pod 的实际 IP
    ports:
      - name: https-metrics
        port: 10257
        protocol: TCP
```





### ELK

![image-20250912220307051](img_%E4%BA%91%E5%8E%9F%E7%94%9F/image-20250912220307051.png)

![image-20250912220319523](img_%E4%BA%91%E5%8E%9F%E7%94%9F/image-20250912220319523.png)