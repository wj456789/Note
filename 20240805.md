### 流程

产品营销工作台前端	产品营销工作台服务端									第三方系统前端		第三方系统服务端

->访问第三方菜单

<-



->调用接口获取票据	->生成ticket

<-返回ticket				<-



->访问第三方页面，在url上带上t，t有效期5分钟，一次性有效		            ->              校验t的合法性 ->

​                                                      <-调用接口，校验t的合法性                                                              <-



​													->t校验成功

​                                                    <-



​													->返回用户信息																		->					





### hbase

```
put 'table','rowKey','columnFamily:columnName','value','columnFamily:columnName','value'
```

### redis

```
redis批量set 使用Pipeline
```

### 执行

```
ProcessBuilder
```

### 定时任务

```
ScheduleRegister获取所有定时任务配置，根据zookeeper对应目录下是否存在任务节点，并行则在zookeeper目录下创建节点且存放任务配置到队列中
20秒循环

ScheduleExecutor定时任务从缓存队列中获取任务配置，使用Scheduler创建job触发
实时循环

ScheduleRebalance判断当前节点的定时任务超过平均值，删除超过个数的定时任务
5分钟循环
```

### maven

```
输出当前项目所有依赖jar包
mvn dependency:copy-dependencies -DoutputDirectory=./lib -DincludeScope=runtime -Dmdep.outputFile=dependencies.txt


```

### java8

```
Java8 并行流(parallelStream)原理分析及注意事项
https://blog.csdn.net/Clearlove_S7/article/details/130183990
```



### SQL

```
MySQL主从AUTO_INCREMENT不一致问题	
replace into 或 insert ... on duplicate key update 操作在表存在自增主键且包含唯一索引的情况下，当出现数据冲突的时候，会触发AUTO_INCREMENT在主从节点的不一致，一旦主从发生切换，就会造成业务写入报主键冲突的错误。
解决建议：
避免使用replace into或使用MySQL8.0以上版本
```



```
创建分区表索引
1、创建表
创建分区表时，需要指定一个非空字段作为分区字段，这里指定一个时间字段作为分区字段，分区间隔是1个小时，需要注意的是，定义主键需要增加本地索引的约束，否则会报错。因为分区表的索引都是本地索引，主键会自动创建一个索引，因此也必须是本地索引。
CREATE TABLE IF NOT EXISTS tabName (
	TASKID VARCGAR(128),
	STARTTIME BINARY_BIGINT NOT NULL,
	ENDTIME BINARY_BIGINT NOT NULL,
	MOIID VARCGAR(512) NOT NULL,
	...
	CONSTRAINT PK_tabName PRIMARY KEY(TASKID,STARTTIME,ENDTIME,MOIID) USING INDEX LOCAL
)
PARTITION BY RANGE (STARTTIME) INTERVAL(3600)
(
	PARTITION p_default VALUES LESS THAN
	(
		TIMESTAMPDIFF(
			SECOND,TO_TIMESTAMP('1970-01-01 00:00:00','YYYY-MM-DD HH24:MI:SS'),
			SYS_EXTRACT_UTC(TO_TIMESTAMP('1970-01-01 00:00:00','YYYY-MM-DD HH24:MI:SS'))
		)
	) 
)

2、创建索引
分区表创建索引必须加上LOCAL字段，同事注意索引的名字必须全局唯一，不能重复
CREATE INDEX IF NOT EXISTS $indexname ON $tablename ($columnname) LOCAL

3、分区查询
从adm_tab_patitions查找指定表的分区
SELECT partition_name,high_value FROM adm_tab_partitions WHERE table_name = $tablename

查询每个分区数据量
SELECT COUNT(*) FROM $tablename PARTITION ($partitionname)

4、分区老化
ALTER TABLE $tablename DROP PARTITION $partitionname

分区老化有两种处理
1）直接删除7天以前的数据分区
2）某个表数据量超过阈值限制，当前配置为1亿条，就循环删除最老的分区，直到数据量在阈值以内
```

```
这段SQL代码是在定义一个分区表，使用的是基于范围的分区策略。这里的PARTITION BY RANGE (STARTTIME) INTERVAL(3600)表示根据STARTTIME字段的值来创建分区，每个分区覆盖3600秒（即1小时）的时间范围。

在分区定义中，有一个特殊的分区p_default，它是一个“默认分区”，用于存储那些不满足其他任何分区条件的数据。这里的条件是VALUES LESS THAN后面跟着的计算结果。

计算过程如下：

TO_TIMESTAMP('1970-01-01 00:00:00','YYYY-MM-DD HH24:MI:SS')将字符串'1970-01-01 00:00:00'转换为时间戳。
SYS_EXTRACT_UTC函数提取时间戳的UTC时间。
TIMESTAMPDIFF(SECOND, ..., ...)计算两个时间戳之间的秒数差。

这个计算的结果是一个整数，表示从1970年1月1日0时0分0秒（UTC时间）到同一时刻（实际上是同一个时间点，所以差值为0）的秒数差。由于差值为0，这个p_default分区实际上是用来存储那些STARTTIME值小于或等于1970年1月1日0时0分0秒UTC的记录。

简而言之，这段SQL定义了一个按小时分区的表，其中p_default分区用于存储时间戳小于或等于1970年1月1日0时0分0秒UTC的记录。
```



```
分区表不用AUTO_INCREMENT，自增主键会自动创建一个全局索引，全局唯一索引在增加或删除分区时候会失效，导致分区表不可用

1、创建主键使用using index local固化
create table $tablename (
	id bigint not null,
	create_time DATETIME not null,
	...
)
partition by range(create_time)(
	partition beforedata values less than (to_date('20220101','yyyymmdd')),
	partition P20220101 values less than (to_date('20220102','yyyymmdd')),
	partition P20220102 values less than (to_date('20220103','yyyymmdd')),
)

alter table $tablename add constraint $indexname primary key(id,create_time) using index local;
alter table $tablename modify id auto_increment;
alter table $tablename anto_increment = 1;

2、创建序列
create sequence $sequencename
increment by 1
start with 1000
maxvalue 9999
minvalue 0
cycle;

insert into $table values($sequencename.nextval, 'abc');
```













```sql
select a.*,sum(b.total_amount) as total from users a 
left join orders b on a.user_id=b.user_id 
group by a.user_id;

普通索引

覆盖索引
create index idx_orders_total_amount_user_id on orders(total_amount,user_id);
create index idx_orders_user_id_total_amount on orders(user_id,total_amount);

减少数据量
select a.*,sum(b.total_amount) as total from users a 
left join orders b on a.user_id=b.user_id 
where a.user_id > 1033 group by a.user_id;

小表驱动大表
select a.*,sum(b.total_amount) as total from users a 
left join (select user_id,total_amount from orders c where c.user_id >1033) b on a.user_id=b.user_id 
where a.user_id >1033 group by a.user_id;

in索引失效，进行强制索引
select a.*,sum(b.total_amount) as total from users a 
left join orders b force index(idx_orders_user_id_total_amount) on a.user_id=b.user_id 
where b.user_id in (1033,1034,1035,1036)

数据达到千万级，需要分表，比如按照用户id取模分表
```











```sql
--创建表空间
create tablespace {0} datafile '{1}' size 6124M autoextend on next 1024M;

--创建用户
create user {0} identified by '{1}' default tablespace {2};

--用户赋权
grant 
create session,
create trigger,drop any trigger,
create table, create view,
create sequence,drop any sequence,
drop any table,
drop any view,
create procedure,drop any procedure,
create any index,drop any index 
to {0};

-- 设置密码有效期
alter profile default limit password_life_time 90;

--设置最大连接数
alter profile default limit sessions_per_user 1024;

--用户赋予select DB_USERS权限
grant select on adm_users to {0};

--赋予用户select adm_tablespaces权限
grant select on adm_tablespaces to {0};
```











### 多线程

```
线程池的核心线程数，如果服务时cpui密集或io密集，分别设置为多少?


线程池的核心线程数设置取决于任务的类型，即CPU密集型还是IO密集型。以下是针对这两种类型的任务分别设置核心线程数的建议：

CPU密集型任务
对于CPU密集型任务，线程池的核心线程数设置应旨在最大化CPU的利用率，同时避免过多的线程上下文切换带来的开销。在这种情况下，核心线程数的设置可以有以下几种考虑：

等于CPU核心数：这是最基础且常见的设置方式，可以确保每个CPU核心都有一个线程在执行任务，从而充分利用CPU资源。
CPU核心数+1：这种设置方式在CPU核心数的基础上额外增加一个线程，主要是为了应对线程执行过程中可能发生的缺页中断或其他异常导致的线程阻塞情况。这样，当某个线程因为阻塞而无法继续执行时，额外的线程可以接替其工作，继续利用CPU资源。
CPU核心数的1.5倍到2倍：这种设置方式在一些情况下可能更加合适，尤其是当任务的CPU占用率不是非常高，或者线程的执行时间相对较短时。通过增加核心线程数，可以进一步提高CPU的利用率，但需要注意不要设置得过多，以免引入过多的线程上下文切换开销。
IO密集型任务
对于IO密集型任务，线程池的核心线程数设置应旨在减少线程的等待时间，提高系统的吞吐量。由于IO操作通常比CPU操作耗时更长，因此可以通过增加线程数来利用等待IO操作的时间，让更多的线程在CPU上执行。在这种情况下，核心线程数的设置可以有以下几种考虑：

2倍CPU核心数：这是一种常见的设置方式，可以确保在IO操作等待期间，有足够多的线程在CPU上执行，从而提高系统的吞吐量。
根据IO等待时间和预期并发量设置：更精确的设置方式是根据实际的IO等待时间和预期的并发量来计算核心线程数。例如，可以使用公式“线程数 = CPU核心数 * (1 + 线程等待时间 / 线程运行总时间)”来估算。
总结
CPU密集型任务：核心线程数可以设置为等于CPU核心数、CPU核心数+1或CPU核心数的1.5倍到2倍，具体取决于任务的CPU占用率和执行时间。
IO密集型任务：核心线程数通常设置为2倍CPU核心数或根据IO等待时间和预期并发量来设置，以充分利用等待IO操作的时间。
```



### zookeeper

```java
import org.apache.curator.framework.CuratorFramework;  
import org.apache.curator.framework.CuratorFrameworkFactory;  
import org.apache.curator.retry.ExponentialBackoffRetry;  
import java.util.Collections;  

public class CuratorExample {  
    public static void main(String[] args) {  
        // 创建重试策略  
        ExponentialBackoffRetry retryPolicy = new ExponentialBackoffRetry(1000, 3);  
  
        // 认证信息，这里以"digest"方式为例，格式为"username:password"的Base64编码  
        String authInfo = "dGVzdDpdGVzdDEyMzN="; // 示例："test:test1234"的Base64编码  
  
        // 使用builder构建CuratorFramework实例  
        // 注意：移除了 .aclProvider(aclProvider)，因为这不是一个有效的配置选项  
        CuratorFramework client = CuratorFrameworkFactory.builder()  
                .connectString("localhost:2181") // ZooKeeper地址  
                .retryPolicy(retryPolicy)  
                .authorization("digest", authInfo.getBytes()) // 配置认证信息  
                .sessionTimeoutMs(30000) // 会话超时时间  
                .connectionTimeoutMs(10000) // 连接超时时间  
                .build();  
  
        // 启动客户端  
        client.start();  
  
        // 现在可以使用client进行ZooKeeper操作了  
        // ...  
  
        // 关闭客户端  
        client.close();  
    }  
}

.aclProvider(aclProvider)
```

### File

```java
import org.apache.commons.io.FileUtils;  
import org.apache.commons.io.LineIterator;  
  
import java.io.File;  
import java.io.IOException;  
  
public class FileUtilsExample {  
    public static void main(String[] args) {  
        File file = new File("path/to/your/file.txt");  
  
        try (LineIterator it = FileUtils.lineIterator(file, "UTF-8")) { // 指定文件的字符编码  
            while (it.hasNext()) {  
                String line = it.nextLine();  
                // 处理每一行  
                System.out.println(line);  
            }  
        } catch (IOException e) {  
            e.printStackTrace();  
        }  
    }  
}
```

### Excel

[【EasyExcel&Hutool】excel表格的导入和导出，csv文件的导入导出](https://blog.csdn.net/weixin_44823875/article/details/133161604)



```
./jstat -gcutil 37783 2s 6000
```





### Http

```java
import org.apache.http.client.config.RequestConfig;  
import org.apache.http.client.methods.CloseableHttpResponse;  
import org.apache.http.client.methods.HttpGet;  
import org.apache.http.impl.client.CloseableHttpClient;  
import org.apache.http.impl.client.HttpClients;  
import org.apache.http.impl.conn.PoolingHttpClientConnectionManager;  
import org.apache.http.util.EntityUtils;  
  
public class HttpClientUtils {  
	private static volatile CloseableHttpClient client;
	private static RequestConfig requestConfig;
	private static PoolingHttpClientConnectionManager manage = new PoolingHttpClientConnectionManager(); 
	private static final int MAX_TOTAL = 150;
	private static final int DEFAULT_MAX_PER_ROUTE = 50;
	static {
		// http连接配置
        requestConfig = RequestConfig.custom()
            .setResponseTimeout(3000, TimeUnit.MILLISECONDS)	// 设置响应超时时间 
            .setConnectionRequestTimeout(3000, TimeUnit.MILLISECONDS) // 从连接池获取连接的超时时间  
            .build();
        // 配置最大连接数
        manage.setMaxTotal(MAX_TOTAL);
        // 每个路由最大连接数
        manage.setDefaultMaxPerRoute(DEFAULT_MAX_PER_POUTE);
        manage.setDefaultConnectionConfig(ConnectionConfig.custom().setConnectTimeout(3000,TimeUnit.MILLISECONDS).build());
	}
	
    public static CloseableHttpClient getHttpClient() {  
        if (client == null) {
            synchronized(HttpClientUtils.class){
                if (client == null) {
                    client = HttpClientBuilder.create()
                        .setConnectionManager(manage)
                        .setDefaultRequestConfig(requestConfig)
                        //清理过期和空闲连接
                        .evictExpiredConnections()
                        .evictldleConnections(TimeValue.of(30000,TimeUnit.MILLISECONDS))
                        .build();
                }
            }
        }
        return client;
    }  
}
```



### Linux



```shell
# 在1111之前添加AAA
$ sed -i 's/1111/AAA&/' /tmp/input.txt

# 在1111之后添加BBB
$ sed -i 's/1111/&BBB/' /tmp/input.txt
```

```shell
# 在每行的头添加字符，比如“HEAD”，命令如下：
$ sed -i 's/^/HEAD$/' tmp/input.txt

# 在每行的尾部添加字符，比如“tail”
$ sed -i 's/$/&tail/' /tmp/input.txt
```

```shell
#要求：(1) 删除所有空行； (2)一行中，如果包含“1111”，则在“1111”前面插入“AAA”，在“11111”后面插入“BBB”
$ sed '/^$/d;s/1111/AAA&/;s/1111/&BBB/' /tmp/input.txt
```





```shell
export default[ 
{
path:"/plan/5g-layout",
name:"scripted.plan.5g-layout"
},
...
]





function comment_func() {
	sed -i 's/export default\[/$\
{\
path:'\"\/plan\/5g-layout'\",\
name:'\"scripted.plan.5g-layout'\"\
},/' /tmp/input.txt

	sed -i '/export default \[/i'"$1" /tmp/input.txt
}
comment_func "import recomment from '@/views/scriptedMarketingPlan/5g-layout.vue'"

# 防止$a存在特殊字符，使用双引号
$ sed -i 's/start=\"\"/start=\"' "$a" '\"/g' 1.txt
```



```shell
$ echo $line|awk -F "@@" '{print $3}'
# -F "@@"标示使用'@@'作为分隔符，$3 标示输出第三个字段
# {}用于包含命令序列，单引号用于引用字符串
```





































